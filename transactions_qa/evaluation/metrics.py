"""
Defines different metrics used for evaluation of tasks.
"""
import re
import math
import scipy
import sklearn
import numpy as np
from filelock import FileLock

import torch
from sacrebleu import corpus_bleu
from rouge_score import rouge_scorer, scoring
from typing import (Callable, Dict, List, Tuple,
                    Iterable, Any, Optional)


try:
    import nltk
    NLTK_AVAILABLE = True
except (ImportError, ModuleNotFoundError):
    NLTK_AVAILABLE = False

if NLTK_AVAILABLE:
    with FileLock(".lock") as lock:
        nltk.download("punkt", quiet=True)


def lmap(f: Callable, x: Iterable) -> List:
    """
    list(map(f, x))
    """
    return list(map(f, x))



def topk_accuracy(output, target, padding, topk=(1,)):
    """
    Computes the accuracy over the k top predictions for the specified values of k
    """
    with torch.no_grad():
        maxk = max(topk)
        if output.shape[-1] < maxk:
            print(f"[WARNING] Less than {maxk} predictions available. Using {output.shape[-1]} for topk.")

        maxk = min(maxk, output.shape[-1])
        batch_size = target.size(0)

        # Take topk along the last dimension.
        _, pred = output.topk(maxk, -1, True, True)  # (N, T, topk)

        mask = (target != padding).type(target.dtype)
        target_expand = target[..., None].expand_as(pred)
        correct = pred.eq(target_expand)
        correct = correct * mask[..., None].expand_as(correct)

        res = []
        for k in topk:
            correct_k = correct[..., :k].reshape(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / mask.sum()))
        return res


ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]


def calculate_rouge(
    predictions: List[str],
    targets: List[str],
    use_stemmer=True,
    rouge_keys=ROUGE_KEYS,
    return_precision_and_recall=False,
    bootstrap_aggregation=True,
    newline_sep=True,
) -> Dict:
    """Calculate rouge using rouge_scorer package.
    Args:
        predictions: list of predictiona generated by model
        targets: list of groundtruth texts (e.g. contents of val.target)
        use_stemmer:  Bool indicating whether Porter stemmer should be used to
        strip word suffixes to improve matching.
        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
        return_precision_and_recall: (False) whether to also return precision and recall.
        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False
            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``
        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL
        on multi sentence summaries (CNN/DM dataset).
    Returns:
         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys
    """

    def add_newline_to_end_of_each_sentence(x: str) -> str:
        """This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS."""
        re.sub("<n>", "", x)  # remove pegasus newline char
        assert NLTK_AVAILABLE, "nltk must be installed to separate newlines between sentences. (pip install nltk)"
        return "\n".join(nltk.sent_tokenize(x))

    def extract_rouge_mid_statistics(dct):
        new_dict = {}
        for k1, v1 in dct.items():
            mid = v1.mid
            new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in ["precision", "recall", "fmeasure"]}
        return new_dict

    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)
    aggregator = scoring.BootstrapAggregator()
    for pred, tgt in zip(targets, predictions):
        # rougeLsum expects "\n" separated sentences within a summary
        if newline_sep:
            pred = add_newline_to_end_of_each_sentence(pred)
            tgt = add_newline_to_end_of_each_sentence(tgt)
        scores = scorer.score(pred, tgt)
        aggregator.add_scores(scores)

    if bootstrap_aggregation:
        result = aggregator.aggregate()
        if return_precision_and_recall:
            return extract_rouge_mid_statistics(result)  # here we return dict
        else:
            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}
    else:
        return aggregator._scores  # here we return defaultdict(list)


def calculate_bleu(predictions, targets, **kwargs) -> dict:
    """
    Uses sacrebleu's corpus_bleu implementation.
    """
    return {"bleu": round(corpus_bleu(predictions, [targets], **kwargs).score, 4)}


def accuracy(predictions, targets) -> dict:
    """
    Computes the average accuracy.
    """
    print("predictions generated for accuracy score", predictions[:5])
    print("targets generated for accuracy score", targets[:5])
    print("shape acc", len(targets))
    return {"acc": 100 * ((np.array(predictions) == np.array(targets)).mean())}


def pearson_corr_coef(predictions, targets) -> dict:
    """
    Computes Pearson correlation coefficient.
    """
    pearson_corrcoef = 100 * scipy.stats.pearsonr(targets, predictions)[0]

    # Note that if all the predictions will be the same, spearman
    # correlation is nan, to gaurad against this, we check the output
    # and return 0 in this case.
    if math.isnan(pearson_corrcoef):
        pearson_corrcoef = 0
    return {"pearson_corrcoef": pearson_corrcoef}


def spearman_corrcoef(predictions, targets) -> dict:
    """
    Computes Spearman correlation coefficient.
    """
    spearman_corrcoef = 100 * scipy.stats.spearmanr(targets, predictions)[0]

    # Note that if all the predictions will be the same, spearman
    # correlation is nan, to gaurad against this, we check the output
    # and return 0 in this case.
    if math.isnan(spearman_corrcoef):
        spearman_corrcoef = 0
    return {"spearman_corrcoef": spearman_corrcoef}


def f1_score_with_invalid(predictions, targets) -> dict:
    """
    Computes F1 score,  with any prediction != 0 or 1 is counted as incorrect.
    Args:
      targets: list of targets, either 0 or 1
      predictions: list of predictions, any integer value
    Returns:
      F1 score, where any prediction != 0 or 1 is counted as wrong.
    """
    targets, predictions = np.asarray(targets), np.asarray(predictions)
    # Get indices of invalid predictions.
    invalid_idx_mask = np.logical_and(predictions != 0, predictions != 1)
    # For any prediction != 0 or 1, we set the prediction to the opposite of its corresponding target.
    predictions[invalid_idx_mask] = 1 - targets[invalid_idx_mask]
    return {"f1": 100 * sklearn.metrics.f1_score(targets, predictions)}


def avg_f1_score_with_invalid(predictions, targets, label_list: Optional[Any] = ["0", "1"]) -> dict:
    """
        Computes F1 score,  with any prediction != 0 or 1 is counted as incorrect.
        Args:
          targets: list of targets, either 0 or 1
          predictions: list of predictions, any integer value
        Returns:
          F1 score, where any prediction != 0 or 1 is counted as wrong.
        """
    targets, predictions = np.asarray(targets), np.asarray(predictions)
    # Get indices of invalid predictions.
    invalid_idx_mask = ~np.isin(predictions, label_list)

    # For any prediction not in list, we set the prediction to the incorrect.
    for j in np.where(invalid_idx_mask)[0].tolist():
        predictions[j] = label_list[(label_list.index(targets[j]) + 1) % len(label_list)]
    return {"f1": 100 * sklearn.metrics.f1_score(targets, predictions, average='macro')}


def matthews_corrcoef(predictions, targets) -> dict:
    """
    Computes the Matthews correlation coefficient.
    """
    return {"mcc": 100 * sklearn.metrics.matthews_corrcoef(targets, predictions)}
