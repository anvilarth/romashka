{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from models import TransactionsModel\n",
    "from data_generators import batches_generator, cat_features_names, num_features_names, meta_features_names\n",
    "\n",
    "from embedding import EmbeddingLayer\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.frames.bert import MLMPretrainModule\n",
    "\n",
    "from ptls.nn import RnnSeqEncoder\n",
    "from ptls.nn import TransformerEncoder\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assets/num_embedding_projections.pkl', 'rb') as f:\n",
    "    num_embedding_projections = pickle.load(f)\n",
    "    \n",
    "with open('./assets/cat_embedding_projections.pkl', 'rb') as f:\n",
    "    cat_embedding_projections = pickle.load(f)\n",
    "\n",
    "with open('./assets/meta_embedding_projections.pkl', 'rb') as f:\n",
    "    meta_embedding_projections = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddedBatch:\n",
    "    def __init__(self, data, mask):\n",
    "        self.payload = data\n",
    "        self.seq_lens = torch.LongTensor([data.shape[1]] * data.shape[0]).to(device)\n",
    "        self.seq_len_mask = mask\n",
    "        \n",
    "class IterDataset(IterableDataset):\n",
    "    def __init__(self, dataset_train, batch_size=64, device='cuda'):\n",
    "        self.data = dataset_train\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.foo = lambda: batches_generator(self.data, batch_size=self.batch_size, shuffle=True, device=self.device, is_train=True, output_format='torch', min_seq_len=200)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = '/home/jovyan/afilatov/data/alfa/train_buckets'\n",
    "\n",
    "dir_with_datasets = os.listdir(path_to_dataset)\n",
    "dataset_train = sorted([os.path.join(path_to_dataset, x) for x in dir_with_datasets])[0:1]\n",
    "\n",
    "#train_dataloader = batches_generator(dataset_train, batch_size=64, shuffle=True,\n",
    "#                                            device=device, is_train=True, output_format='torch', min_seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IterDataset(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PtlsEmbeddingLayer(EmbeddingLayer):\n",
    "    def __init__(self, splitter, *args, **kwargs):\n",
    "        self.splitter = splitter\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_size = self.get_embedding_size()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = x['mask']\n",
    "        x = super().forward(x)\n",
    "        return PaddedBatch(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySampleUniform:\n",
    "    \"\"\"\n",
    "    Sub samples with equal length = `seq_len`\n",
    "    Start pos has fixed uniform distribution from sequence start to end with equal step\n",
    "    |---------------------|       main sequence\n",
    "    |------|              |        sub seq 1\n",
    "    |    |------|         |        sub seq 2\n",
    "    |         |------|    |        sub seq 3\n",
    "    |              |------|        sub seq 4\n",
    "    There is no random factor in this splitter, so sub sequences are the same every time\n",
    "    Can be used during inference as test time augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, split_count, seq_len, **_):\n",
    "        self.split_count = split_count\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def split(self, dates):\n",
    "        date_len = dates.shape[0]\n",
    "        date_range = np.arange(date_len)\n",
    "\n",
    "        if date_len <= self.seq_len + self.split_count:\n",
    "            return [date_range for _ in range(self.split_count)]\n",
    "\n",
    "        start_pos = np.linspace(0, date_len - self.seq_len, self.split_count).round().astype(int)\n",
    "        return [date_range[s:s + self.seq_len] for s in start_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_features', 'cat_features', 'mask', 'event_time', 'meta_features', 'label', 'app_id'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def split_process(batch, splitter):\n",
    "    res = {}\n",
    "    \n",
    "    local_date = batch['event_time']\n",
    "    if splitter is not None:\n",
    "        indexes = splitter.split(local_date)\n",
    "        pad_size = max([len(ixs) for ixs in indexes])\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        if type(v) == list and len(v) > 1 and splitter is not None:\n",
    "            new_v = []\n",
    "            for elem in v:\n",
    "                tmp = []\n",
    "                for i, ixs in enumerate(indexes):\n",
    "                    to_tmp = elem[:, ixs]\n",
    "                    if to_tmp.shape[1] < pad_size:\n",
    "                        to_tmp = torch.cat([\n",
    "                            to_tmp, torch.zeros(to_tmp.shape[0], pad_size - to_tmp.shape[1]).to(device)\n",
    "                        ], axis=1)\n",
    "                    tmp.append(to_tmp)\n",
    "                new_v.append(torch.cat(tmp, dim=0))\n",
    "        else:\n",
    "            new_v = v \n",
    "        res[k] = new_v\n",
    "    return res\n",
    "\n",
    "def replace_token(batch, replace_prob=0.15, skip_first=1):\n",
    "    mask = batch['mask']\n",
    "    to_replace = torch.bernoulli(mask * replace_prob).bool()\n",
    "    to_replace[:, :skip_first] = False\n",
    "\n",
    "    sampled_trx_ids = torch.multinomial(\n",
    "        mask.flatten().float(),\n",
    "        num_samples=to_replace.sum().item(),\n",
    "        replacement=True,\n",
    "    )\n",
    "\n",
    "    to_replace_flatten = to_replace.flatten()\n",
    "    new_x = deepcopy(batch)\n",
    "    for k, v in new_x.items():\n",
    "        if type(v) == list and len(v) > 1:\n",
    "            for elem in v:\n",
    "                elem.flatten()[to_replace_flatten] = elem.flatten()[sampled_trx_ids]\n",
    "    return new_x, to_replace.long().flatten()#[mask.flatten().bool()]\n",
    "\n",
    "\n",
    "def my_collate_fn(batch, splitter, rep=5, mode='coles'):\n",
    "    batch = batch[0]\n",
    "    len_batch = batch['num_features'][0].shape[0]\n",
    "    labels = torch.arange(len_batch).repeat(rep)\n",
    "    batch = split_process(batch, splitter)\n",
    "    \n",
    "    if mode == 'coles':\n",
    "        return batch, labels\n",
    "    \n",
    "    if mode == 'cpc':\n",
    "        return batch, None\n",
    "    \n",
    "    if mode == 'rtd':\n",
    "        batch, labels = replace_token(batch)\n",
    "        return batch, labels\n",
    "        \n",
    "    if mode == 'mlm':\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames import PtlsDataModule\n",
    "\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from ptls.data_load.utils import collate_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coles_splitter = MySampleUniform(\n",
    "        split_count=5,\n",
    "        seq_len=100\n",
    "    )\n",
    "coles_splitter = SampleSlices(split_count=5, cnt_min=50, cnt_max=100)\n",
    "\n",
    "coles_ptls_emb_layer = PtlsEmbeddingLayer(coles_splitter,\n",
    "                                    cat_embedding_projections,\n",
    "                                    cat_features_names,\n",
    "                                    num_embedding_projections,\n",
    "                                    num_features_names).cuda()\n",
    "\n",
    "coles_seq_encoder = RnnSeqEncoder(\n",
    "    input_size=coles_ptls_emb_layer.get_embedding_size(),\n",
    "    trx_encoder=coles_ptls_emb_layer,\n",
    "    hidden_size=256,\n",
    "    type='gru',\n",
    ")\n",
    "\n",
    "coles_model = CoLESModule(\n",
    "    seq_encoder=coles_seq_encoder,\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.9),\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coles_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=partial(my_collate_fn, splitter=coles_splitter, rep=5, mode='coles'),\n",
    "    num_workers=0,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import logging\n",
    "\n",
    "coles_trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    gpus=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | ContrastiveLoss | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 352 K \n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "352 K     Trainable params\n",
      "0         Non-trainable params\n",
      "352 K     Total params\n",
      "1.409     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf08aa3cdf7549578b70abe3f85387c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "coles_trainer.fit(coles_model, coles_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.cpc import CpcModule\n",
    "from ptls.frames.coles.split_strategy import SampleSlices, SampleUniformBySplitCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_splitter = SampleUniformBySplitCount(split_count=5) # splitter should preserve order in samples\n",
    "cpc_splitter = SampleSlices(split_count=5, cnt_min=50, cnt_max=100, is_sorted=True)\n",
    "\n",
    "cpc_ptls_emb_layer = PtlsEmbeddingLayer(cpc_splitter,\n",
    "                                        cat_embedding_projections,\n",
    "                                        cat_features_names,\n",
    "                                        num_embedding_projections,\n",
    "                                        num_features_names).cuda()\n",
    "\n",
    "cpc_seq_encoder = RnnSeqEncoder(\n",
    "    input_size=cpc_ptls_emb_layer.get_embedding_size(),\n",
    "    trx_encoder=cpc_ptls_emb_layer,\n",
    "    hidden_size=256,\n",
    "    type='gru',\n",
    ")\n",
    "\n",
    "cpc_model = CpcModule(\n",
    "    seq_encoder=cpc_seq_encoder,\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.9)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=partial(my_collate_fn, splitter=cpc_splitter, mode='cpc'),\n",
    "    num_workers=0,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "cpc_trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    gpus=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type          | Params\n",
      "-----------------------------------------------------\n",
      "0 | _loss              | CPC_Loss      | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder | 352 K \n",
      "2 | _validation_metric | CpcAccuracy   | 0     \n",
      "3 | _linears           | ModuleList    | 280 K \n",
      "-----------------------------------------------------\n",
      "632 K     Trainable params\n",
      "0         Non-trainable params\n",
      "632 K     Total params\n",
      "2.532     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f7447e801e4fc58369c35d4d4fdcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%debug\n",
    "cpc_trainer.fit(cpc_model, cpc_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from ptls.frames.bert import RtdModule\n",
    "from ptls.nn.seq_encoder.utils import AllStepsHead, FlattenHead\n",
    "from ptls.frames.coles.split_strategy import SampleUniformBySplitCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rtd_splitter = SampleUniformBySplitCount(split_count=5)\n",
    "rtd_ptls_emb_layer = PtlsEmbeddingLayer(None,\n",
    "                                        cat_embedding_projections,\n",
    "                                        cat_features_names,\n",
    "                                        num_embedding_projections,\n",
    "                                        num_features_names).cuda()\n",
    "\n",
    "rtd_seq_encoder = RnnSeqEncoder(\n",
    "    input_size=rtd_ptls_emb_layer.get_embedding_size(),\n",
    "    trx_encoder=rtd_ptls_emb_layer,\n",
    "    hidden_size=256,\n",
    "    type='gru',\n",
    ").cuda()\n",
    "\n",
    "rtd_model = RtdModule(\n",
    "    seq_encoder=rtd_seq_encoder,\n",
    "    validation_metric=torchmetrics.AUROC(task='binary'),\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.9),\n",
    "    head = torch.nn.Sequential(\n",
    "        AllStepsHead(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(256, 1),\n",
    "                torch.nn.Sigmoid(),\n",
    "                torch.nn.Flatten(),\n",
    "            )\n",
    "        ),\n",
    "        FlattenHead(),\n",
    "    )\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtd_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=partial(my_collate_fn, splitter=None, mode='rtd', rep=1),\n",
    "    num_workers=0,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "rtd_trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    gpus=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type          | Params\n",
      "-----------------------------------------------------\n",
      "0 | _loss              | BCELoss       | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder | 352 K \n",
      "2 | _validation_metric | BinaryAUROC   | 0     \n",
      "3 | _head              | Sequential    | 257   \n",
      "-----------------------------------------------------\n",
      "352 K     Trainable params\n",
      "0         Non-trainable params\n",
      "352 K     Total params\n",
      "1.410     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061704492fd44a768a51bef3c70da165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%debug\n",
    "rtd_trainer.fit(rtd_model, rtd_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.bert import MLMPretrainModule\n",
    "from ptls.nn import RnnEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_ptls_emb_layer = PtlsEmbeddingLayer(None,\n",
    "                                        cat_embedding_projections,\n",
    "                                        cat_features_names,\n",
    "                                        num_embedding_projections,\n",
    "                                        num_features_names).cuda()\n",
    "\n",
    "mlm_seq_encoder = RnnEncoder(\n",
    "    #trx_encoder=mlm_ptls_emb_layer,\n",
    "    input_size=mlm_ptls_emb_layer.get_embedding_size(),\n",
    "    is_reduce_sequence=False,\n",
    "    hidden_size=182,\n",
    "    type='gru',\n",
    ").cuda()\n",
    "\n",
    "mlm_model = MLMPretrainModule(\n",
    "    trx_encoder=mlm_ptls_emb_layer, \n",
    "    seq_encoder=mlm_seq_encoder,\n",
    "    total_steps=10000\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=partial(my_collate_fn, splitter=None, rep=1, mode='mlm'),\n",
    "    num_workers=0,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "mlm_trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    gpus=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | trx_encoder     | PtlsEmbeddingLayer | 14.1 K\n",
      "1 | _seq_encoder    | RnnEncoder         | 200 K \n",
      "2 | fn_norm_predict | PBShell            | 0     \n",
      "3 | loss_fn         | QuerySoftmaxLoss   | 0     \n",
      "4 | train_mlm_loss  | MeanMetric         | 0     \n",
      "5 | valid_mlm_loss  | MeanMetric         | 0     \n",
      "-------------------------------------------------------\n",
      "214 K     Trainable params\n",
      "0         Non-trainable params\n",
      "214 K     Total params\n",
      "0.857     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0605ca89657d4303819431cceb028a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlm_trainer.fit(mlm_model, mlm_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "83c3bc64cdb2fa57c90e4138bece6a48e4148d75db176957f4eccfc04658c255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
