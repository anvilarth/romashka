{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from models import TransactionsModel\n",
    "from data_generators import batches_generator, cat_features_names, num_features_names, meta_features_names\n",
    "\n",
    "from embedding import EmbeddingLayer\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.frames.bert import MLMPretrainModule\n",
    "from ptls.frames.coles import CoLESModule, ColesIterableDataset\n",
    "\n",
    "from ptls.nn import TransformerEncoder\n",
    "from functools import partial\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assets/num_embedding_projections.pkl', 'rb') as f:\n",
    "    num_embedding_projections = pickle.load(f)\n",
    "    \n",
    "with open('./assets/cat_embedding_projections.pkl', 'rb') as f:\n",
    "    cat_embedding_projections = pickle.load(f)\n",
    "\n",
    "with open('./assets/meta_embedding_projections.pkl', 'rb') as f:\n",
    "    meta_embedding_projections = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddedBatch:\n",
    "    def __init__(self, data):\n",
    "        self.payload = data\n",
    "        self.seq_lens = [data.shape[1]] * data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterDataset(IterableDataset):\n",
    "    def __init__(self, dataset_train, batch_size=64, device='cuda'):\n",
    "        self.data = dataset_train\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.foo = lambda: batches_generator(self.data, batch_size=self.batch_size, shuffle=True, device=self.device, is_train=True, output_format='torch', min_seq_len=200)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = '/home/jovyan/data/alfa/train_buckets'\n",
    "\n",
    "\n",
    "dir_with_datasets = os.listdir(path_to_dataset)\n",
    "dataset_train = sorted([os.path.join(path_to_dataset, x) for x in dir_with_datasets])[0:1]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataloader = batches_generator(dataset_train, batch_size=64, shuffle=True,\n",
    "                                            device=device, is_train=True, output_format='torch', min_seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IterDataset(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.frames.coles import ColesDataset, ColesIterableDataset\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames import PtlsDataModule\n",
    "\n",
    "from ptls.data_load.utils import collate_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySampleUniform:\n",
    "    \"\"\"\n",
    "    Sub samples with equal length = `seq_len`\n",
    "    Start pos has fixed uniform distribution from sequence start to end with equal step\n",
    "    |---------------------|       main sequence\n",
    "    |------|              |        sub seq 1\n",
    "    |    |------|         |        sub seq 2\n",
    "    |         |------|    |        sub seq 3\n",
    "    |              |------|        sub seq 4\n",
    "    There is no random factor in this splitter, so sub sequences are the same every time\n",
    "    Can be used during inference as test time augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, split_count, seq_len, **_):\n",
    "        self.split_count = split_count\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def split(self, dates):\n",
    "        date_len = dates.shape[0]\n",
    "        date_range = np.arange(date_len)\n",
    "\n",
    "        if date_len <= self.seq_len + self.split_count:\n",
    "            return [date_range for _ in range(self.split_count)]\n",
    "\n",
    "        start_pos = np.linspace(0, date_len - self.seq_len, self.split_count).round().astype(int)\n",
    "        return [date_range[s:s + self.seq_len] for s in start_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=MySampleUniform(\n",
    "        split_count=5,\n",
    "        seq_len=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coles_process(batch, splitter):\n",
    "    res = {}\n",
    "    \n",
    "    local_date = batch['event_time']\n",
    "    indexes = splitter.split(local_date)\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        if type(v) == list and len(v) > 1:\n",
    "            new_v = []\n",
    "            for elem in v:\n",
    "                tmp = []\n",
    "                for ix in indexes:\n",
    "                    tmp.append(elem[:, ix])\n",
    "                new_v.append(torch.cat(tmp, dim=0))\n",
    "        else:\n",
    "            new_v = v \n",
    "        res[k] = new_v\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coles_collate_fn(batch):\n",
    "    batch = batch[0]\n",
    "    len_batch = batch['num_features'][0].shape[0]\n",
    "    batch = coles_process(batch, splitter)\n",
    "    # print(batch)\n",
    "    labels = torch.arange(len_batch).repeat(5)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=coles_collate_fn,\n",
    "    num_workers=0,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PtlsEmbeddingLayer(EmbeddingLayer):\n",
    "    def __init__(self, splitter, *args, **kwargs):\n",
    "        self.splitter = splitter\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        return PaddedBatch(x)\n",
    "\n",
    "    \n",
    "ptls_emb_layer = PtlsEmbeddingLayer(splitter,\n",
    "                                    cat_embedding_projections,\n",
    "                                    cat_features_names,\n",
    "                                    num_embedding_projections,\n",
    "                                    num_features_names).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.nn import RnnSeqEncoder\n",
    "\n",
    "seq_encoder = RnnSeqEncoder(\n",
    "    input_size=ptls_emb_layer.get_embedding_size(),\n",
    "    trx_encoder=ptls_emb_layer,\n",
    "    hidden_size=256,\n",
    "    type='gru',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoLESModule(\n",
    "    seq_encoder=seq_encoder,\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.9),\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    gpus=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-dev-2-0/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:133: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | ContrastiveLoss | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 352 K \n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "352 K     Trainable params\n",
      "0         Non-trainable params\n",
      "352 K     Total params\n",
      "1.409     Total estimated model params size (MB)\n",
      "/home/jovyan/.imgenv-dev-2-0/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b89621c0e7349cc9b195c2a0422eb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-dev-2-0/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "trainer.fit(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "83c3bc64cdb2fa57c90e4138bece6a48e4148d75db176957f4eccfc04658c255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
