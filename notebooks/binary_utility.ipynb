{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8da87289-e8e4-4de5-b1c4-1f278683bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['savefig.facecolor'] = 'white'\n",
    "plt.rcParams['savefig.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d47bd38-baf2-41c4-8b0d-f6a66c4bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977c59b",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca5a27e-3293-40eb-a1c1-587332db18cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d1654ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transactions_qa.tqa_model import TransactionQAModel\n",
    "from src.models.components.models import TransactionsModel\n",
    "from src.utils.tools import (make_time_batch, \n",
    "                   calculate_embedding_size)\n",
    "\n",
    "from src.data.alfa.components import ( \n",
    "                             cat_features_names, \n",
    "                             num_features_names, \n",
    "                             meta_features_names)\n",
    "\n",
    "from src.data import AlfaDataModule \n",
    "from src.transactions_qa.tqa_model import TransactionQAModel\n",
    "from src.transactions_qa.utils import get_projections_maps, get_exponent_number, get_mantissa_number\n",
    "from src.tasks import AbstractTask, AutoTask\n",
    "from src.transactions_qa.utils import get_split_indices,  prepare_splitted_batch, collate_batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7024fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ebf7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "780f1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transaction_model(encoder_type='whisper/tiny', head_type='next'):\n",
    "    projections_maps = get_projections_maps(relative_folder=\"..\")\n",
    "    # Loading Transactions model & weights\n",
    "    print(f\"Loading Transactions model...\")\n",
    "\n",
    "    transactions_model_encoder_type = encoder_type\n",
    "    transactions_model_head_type = head_type\n",
    "\n",
    "\n",
    "    transactions_model_config = {\n",
    "        \"cat_features\": cat_features_names,\n",
    "        \"cat_embedding_projections\": projections_maps.get('cat_embedding_projections'),\n",
    "        \"num_features\": num_features_names,\n",
    "        \"num_embedding_projections\": projections_maps.get('num_embedding_projections'),\n",
    "        \"meta_features\": meta_features_names,\n",
    "        \"meta_embedding_projections\": projections_maps.get('meta_embedding_projections'),\n",
    "        \"encoder_type\": transactions_model_encoder_type,\n",
    "        \"head_type\": transactions_model_head_type,\n",
    "        \"embedding_dropout\": 0.1\n",
    "    }\n",
    "    transactions_model = TransactionsModel(**transactions_model_config)\n",
    "\n",
    "    return transactions_model, projections_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c32bad2-82bf-454a-901e-c5288226d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_language_model(language_model_name_or_path=\"google/flan-t5-small\"):\n",
    "    use_fast_tokenizer = True\n",
    "\n",
    "    print(f\"Loading Language model: `{language_model_name_or_path}`...\")\n",
    "    config_kwargs = {\n",
    "        \"use_auth_token\": None,\n",
    "        \"return_unused_kwargs\": True\n",
    "    }\n",
    "\n",
    "    tokenizer_kwargs = {\n",
    "        \"use_fast\": use_fast_tokenizer,\n",
    "        \"use_auth_token\": None,\n",
    "        \"do_lowercase\": False\n",
    "    }\n",
    "\n",
    "    config, unused_kwargs = AutoConfig.from_pretrained(\n",
    "        language_model_name_or_path, **config_kwargs\n",
    "    )\n",
    "    # Download vocabulary from huggingface.co and define model-specific arguments\n",
    "    tokenizer = AutoTokenizer.from_pretrained(language_model_name_or_path, **tokenizer_kwargs)\n",
    "\n",
    "    # Download model from huggingface.co and cache.\n",
    "    lm_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        language_model_name_or_path,\n",
    "        config=config\n",
    "    )\n",
    "    return lm_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e61bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datamodule():\n",
    "    DATA_PATH = '/home/jovyan/romashka/data' \n",
    "    dataset_config = {\n",
    "                'data_dir': DATA_PATH,\n",
    "                'batch_size': 32,\n",
    "                'min_seq_len': 0,\n",
    "                'max_seq_len': 250,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 5,\n",
    "                'pin_memory': True,\n",
    "                'seed': 42\n",
    "    }    \n",
    "\n",
    "    dm = AlfaDataModule(**dataset_config)\n",
    "    return dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35580e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks(task_names, tokenizer):\n",
    "    # Create tasks\n",
    "    tasks = []\n",
    "    tasks_kwargs = [{\"num_options\": 6, \"floating_threshold\": True, 'answer2text': False, 'use_numerical_output': False}, \n",
    "    {\"num_options\": 6, \"floating_threshold\": False, 'use_numerical_output': False}] # ground truth + 5 additional options\n",
    "    if isinstance(task_names, str):\n",
    "        task_names = eval(task_names)\n",
    "    if isinstance(tasks_kwargs, str):\n",
    "        tasks_kwargs = eval(tasks_kwargs)\n",
    "    print(f\"Got task_names: {task_names} with task_kwargs: {tasks_kwargs}\")\n",
    "\n",
    "    for task_i, task_name in enumerate(task_names):\n",
    "        task_kwargs = tasks_kwargs[task_i] if task_i < len(tasks_kwargs) else {}\n",
    "        if \"tokenizer\" not in task_kwargs:\n",
    "            task_kwargs['tokenizer'] = tokenizer\n",
    "        task = AutoTask.get(task_name=task_name, **task_kwargs)\n",
    "        tasks.append(task)\n",
    "    print(f\"Created {len(tasks)} tasks.\")\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83d834e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tqa_model(lm_model, transactions_model, tokenizer, tasks):\n",
    "    # Create general Tranactions QA model\n",
    "    max_steps = 100_000\n",
    "    warmup_steps = 1000\n",
    "    do_freeze_transactions_model = True\n",
    "    do_freeze_language_model = True\n",
    "    do_freeze_connector = False\n",
    "\n",
    "    transactionsQA_model_config = {\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"training_steps\": max_steps,\n",
    "        \"do_freeze_tm\": do_freeze_transactions_model,\n",
    "        \"do_freeze_lm\": do_freeze_language_model,\n",
    "        \"do_freeze_connector\": do_freeze_connector,\n",
    "        \"connector_input_size\": 384,\n",
    "        \"use_numerical_input\": False,\n",
    "        \"use_numerical_output\": False,\n",
    "        \"numerical_context\": \"context\",\n",
    "    }\n",
    "\n",
    "    model = TransactionQAModel(\n",
    "        language_model=lm_model,\n",
    "        transaction_model=transactions_model,\n",
    "        tokenizer=tokenizer,\n",
    "        tasks=tasks,\n",
    "        **transactionsQA_model_config\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f61b93c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language model: `google/flan-t5-small`...\n",
      "Loading Transactions model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:27:22,967 - [INFO] - Tasks - (task_abstract.py).generate_question_templates(206) - Given 5 starting options and 1 ending options results in 5 total combinations.\n",
      "2023-06-22 11:27:22,970 - [INFO] - Tasks - (task_abstract.py).extend_vocabulary(173) - Added to tokenizer: 2 tokens.\n",
      "2023-06-22 11:27:22,971 - [INFO] - Tasks - (task_abstract.py).extend_vocabulary(179) - Notice: resize_token_embeddings of a model to adapt to the size of the new vocabulary!\n",
      "2023-06-22 11:27:22,978 - [INFO] - Tasks - (task_abstract.py).generate_question_templates(206) - Given 5 starting options and 1 ending options results in 5 total combinations.\n",
      "2023-06-22 11:27:22,979 - [INFO] - Tasks - (task_abstract.py).extend_vocabulary(173) - Added to tokenizer: 0 tokens.\n",
      "2023-06-22 11:27:22,980 - [INFO] - Tasks - (task_abstract.py).extend_vocabulary(179) - Notice: resize_token_embeddings of a model to adapt to the size of the new vocabulary!\n",
      "2023-06-22 11:27:22,982 - [INFO] - Tasks - (task_abstract.py).generate_question_templates(206) - Given 5 starting options and 5 ending options results in 25 total combinations.\n",
      "2023-06-22 11:27:23,008 - [INFO] - TransactionQAModel - (tqa_model.py).__init__(113) - Setuping metrics.\n",
      "2023-06-22 11:27:23,008 - [INFO] - TransactionQAModel - (tqa_model.py).__init__(113) - Setuping metrics.\n",
      "2023-06-22 11:27:23,008 - [INFO] - TransactionQAModel - (tqa_model.py).__init__(113) - Setuping metrics.\n",
      "2023-06-22 11:27:23,013 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(206) - Running in `single task` settingwith a single task: next_amnt_open_ended provided.\n",
      "2023-06-22 11:27:23,013 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(206) - Running in `single task` settingwith a single task: next_amnt_open_ended provided.\n",
      "2023-06-22 11:27:23,013 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(206) - Running in `single task` settingwith a single task: next_amnt_open_ended provided.\n",
      "2023-06-22 11:27:23,016 - [INFO] - TransactionQAModel - (tqa_model.py)._set_model_type(241) - Language model type: `encoder-decoder`\n",
      "2023-06-22 11:27:23,016 - [INFO] - TransactionQAModel - (tqa_model.py)._set_model_type(241) - Language model type: `encoder-decoder`\n",
      "2023-06-22 11:27:23,016 - [INFO] - TransactionQAModel - (tqa_model.py)._set_model_type(241) - Language model type: `encoder-decoder`\n",
      "2023-06-22 11:27:23,018 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(251) - LM initial `num_embeddings`: 32128, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,018 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(251) - LM initial `num_embeddings`: 32128, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,018 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(251) - LM initial `num_embeddings`: 32128, `embedding_dim`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING whisper\n",
      "Got task_names: ['next_amnt_open_ended'] with task_kwargs: [{'num_options': 6, 'floating_threshold': True, 'answer2text': False, 'use_numerical_output': False}, {'num_options': 6, 'floating_threshold': False, 'use_numerical_output': False}]\n",
      "Created 1 tasks.\n",
      "Got task_names: ['next_amnt_binary'] with task_kwargs: [{'num_options': 6, 'floating_threshold': True, 'answer2text': False, 'use_numerical_output': False}, {'num_options': 6, 'floating_threshold': False, 'use_numerical_output': False}]\n",
      "Created 1 tasks.\n",
      "Output dimension of embedding model: 384\n",
      "Input dimension of autoregressive model: 512\n",
      "Creating linear connector from 384 to 512 and move to device: cpu.\n",
      "ModuleDict(\n",
      "  (next_amnt_open_ended): ModuleDict(\n",
      "    (mae): MeanAbsoluteError()\n",
      "    (mape): MeanAbsolutePercentageError()\n",
      "    (rouge): ROUGEScore()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:27:23,293 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(263) - LM resized `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,293 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(263) - LM resized `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,293 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(263) - LM resized `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,299 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(217) - Freezing transaction model's parameters...\n",
      "2023-06-22 11:27:23,299 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(217) - Freezing transaction model's parameters...\n",
      "2023-06-22 11:27:23,299 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(217) - Freezing transaction model's parameters...\n",
      "2023-06-22 11:27:23,304 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(222) - Freezing language model's parameters...\n",
      "2023-06-22 11:27:23,304 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(222) - Freezing language model's parameters...\n",
      "2023-06-22 11:27:23,304 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(222) - Freezing language model's parameters...\n",
      "2023-06-22 11:27:23,403 - [INFO] - TransactionQAModel - (tqa_model.py).__init__(113) - Setuping metrics.\n",
      "2023-06-22 11:27:23,403 - [INFO] - TransactionQAModel - (tqa_model.py).__init__(113) - Setuping metrics.\n",
      "2023-06-22 11:27:23,403 - [INFO] - TransactionQAModel - (tqa_model.py).__init__(113) - Setuping metrics.\n",
      "2023-06-22 11:27:23,403 - [INFO] - TransactionQAModel - (tqa_model.py).__init__(113) - Setuping metrics.\n",
      "2023-06-22 11:27:23,409 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(206) - Running in `single task` settingwith a single task: next_amnt_binary provided.\n",
      "2023-06-22 11:27:23,409 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(206) - Running in `single task` settingwith a single task: next_amnt_binary provided.\n",
      "2023-06-22 11:27:23,409 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(206) - Running in `single task` settingwith a single task: next_amnt_binary provided.\n",
      "2023-06-22 11:27:23,409 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(206) - Running in `single task` settingwith a single task: next_amnt_binary provided.\n",
      "2023-06-22 11:27:23,413 - [INFO] - TransactionQAModel - (tqa_model.py)._set_model_type(241) - Language model type: `encoder-decoder`\n",
      "2023-06-22 11:27:23,413 - [INFO] - TransactionQAModel - (tqa_model.py)._set_model_type(241) - Language model type: `encoder-decoder`\n",
      "2023-06-22 11:27:23,413 - [INFO] - TransactionQAModel - (tqa_model.py)._set_model_type(241) - Language model type: `encoder-decoder`\n",
      "2023-06-22 11:27:23,413 - [INFO] - TransactionQAModel - (tqa_model.py)._set_model_type(241) - Language model type: `encoder-decoder`\n",
      "2023-06-22 11:27:23,417 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(251) - LM initial `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,417 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(251) - LM initial `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,417 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(251) - LM initial `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,417 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(251) - LM initial `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,423 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(263) - LM resized `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,423 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(263) - LM resized `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,423 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(263) - LM resized `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,423 - [INFO] - TransactionQAModel - (tqa_model.py)._resize_text_embeddings(263) - LM resized `num_embeddings`: 32102, `embedding_dim`: 512\n",
      "2023-06-22 11:27:23,425 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(217) - Freezing transaction model's parameters...\n",
      "2023-06-22 11:27:23,425 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(217) - Freezing transaction model's parameters...\n",
      "2023-06-22 11:27:23,425 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(217) - Freezing transaction model's parameters...\n",
      "2023-06-22 11:27:23,425 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(217) - Freezing transaction model's parameters...\n",
      "2023-06-22 11:27:23,432 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(222) - Freezing language model's parameters...\n",
      "2023-06-22 11:27:23,432 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(222) - Freezing language model's parameters...\n",
      "2023-06-22 11:27:23,432 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(222) - Freezing language model's parameters...\n",
      "2023-06-22 11:27:23,432 - [INFO] - TransactionQAModel - (tqa_model.py)._prepare_model(222) - Freezing language model's parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimension of embedding model: 384\n",
      "Input dimension of autoregressive model: 512\n",
      "Creating linear connector from 384 to 512 and move to device: cpu.\n",
      "ModuleDict(\n",
      "  (next_amnt_binary): ModuleDict(\n",
      "    (auc): BinaryAUROC()\n",
      "    (accuracy): BinaryAccuracy()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# task_names = ['next_transactions_30_days_binary', 'default', 'next_mcc_binary', 'next_mcc_open_ended']\n",
    "task_names1 = ['next_amnt_open_ended']\n",
    "task_names2 = ['next_amnt_binary']\n",
    "\n",
    "LM_NAME = 'google/flan-t5-small'\n",
    "\n",
    "lm_model, tokenizer = load_language_model(language_model_name_or_path=LM_NAME)\n",
    "transactions_model, projections_maps = load_transaction_model()\n",
    "\n",
    "tasks1 = load_tasks(task_names1, tokenizer)\n",
    "tasks2 = load_tasks(task_names2, tokenizer)\n",
    "dm = load_datamodule()\n",
    "\n",
    "tqa_model1 = make_tqa_model(lm_model, transactions_model, tokenizer, tasks1)\n",
    "tqa_model2 = make_tqa_model(lm_model, transactions_model, tokenizer, tasks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "529923c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('/home/jovyan/romashka/checkpoints/checkpoints/tqa_200k-steps_ft=all_numerical_v5/last.ckpt')['state_dict']\n",
    "\n",
    "# tqa_model.load_state_dict(ckpt, strict=False)\n",
    "# tqa_model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e986c",
   "metadata": {},
   "source": [
    "### Numerical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be2549a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import contextlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ended_results = []\n",
    "binary_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(dm.val_dataloader()):\n",
    "        # Removing prints\n",
    "        tqa_model1.eval()\n",
    "        tqa_model2.eval()\n",
    "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "            out1, batch_answers1 = tqa_model1.model_step(batch)\n",
    "            pred1, true_output = tqa_model1.tasks[0].process_outputs(out1, batch_answers1)\n",
    "\n",
    "            out2, batch_answers2 = tqa_model2.model_step(batch)\n",
    "            pred2, true_output2 = tqa_model2.tasks[0].process_outputs(out2, batch_answers2)\n",
    "\n",
    "            questions = tqa_model2.tokenizer.batch_decode(out2['question_encoded'], skip_special_tokens=True)\n",
    "            thresholds = torch.tensor([float(re.findall(\"\\d+\\.\\d+\",  string)[0]) for string in questions])\n",
    "\n",
    "            open_ended_predictions = (pred1 > thresholds) == true_output2\n",
    "            binary_predictions = (pred2 > 0.5) == true_output2\n",
    "\n",
    "            open_ended_results.append(open_ended_predictions)\n",
    "            binary_results.append(binary_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ff2a09b934b25be9a8b176e9d1e80ece927d491d3847c62b1a0ae23e4142515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
