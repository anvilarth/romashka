{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch \n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.transactions_qa.tqa_model import TransactionQAModel\n",
    "from src.models.components.models import TransactionsModel\n",
    "from src.utils.tools import (make_time_batch, \n",
    "                   calculate_embedding_size)\n",
    "\n",
    "from src.data.alfa.components import ( \n",
    "                             cat_features_names, \n",
    "                             num_features_names, \n",
    "                             meta_features_names)\n",
    "\n",
    "from src.data import AlfaDataModule, AlfaPretrainingDataModule\n",
    "from src.transactions_qa.tqa_model import TransactionQAModel\n",
    "from src.transactions_qa.utils import get_projections_maps\n",
    "from src.tasks import AbstractTask, AutoTask\n",
    "\n",
    "from src.models.components.embedding import EmbeddingLayer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.frames.bert import MLMPretrainModule\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from ptls.frames.cpc import CpcModule\n",
    "\n",
    "from ptls.nn import TransformerEncoder\n",
    "from ptls.nn.seq_encoder.containers import SeqEncoderContainer, RnnSeqEncoder\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transaction_model(encoder_type='whisper/tiny', head_type='next'):\n",
    "    projections_maps = get_projections_maps(relative_folder=\"..\")\n",
    "    # Loading Transactions model & weights\n",
    "    print(f\"Loading Transactions model...\")\n",
    "\n",
    "    transactions_model_encoder_type = encoder_type\n",
    "    transactions_model_head_type = head_type\n",
    "\n",
    "\n",
    "    transactions_model_config = {\n",
    "        \"cat_features\": cat_features_names,\n",
    "        \"cat_embedding_projections\": projections_maps.get('cat_embedding_projections'),\n",
    "        \"num_features\": num_features_names,\n",
    "        \"num_embedding_projections\": projections_maps.get('num_embedding_projections'),\n",
    "        \"meta_features\": meta_features_names,\n",
    "        \"meta_embedding_projections\": projections_maps.get('meta_embedding_projections'),\n",
    "        \"encoder_type\": transactions_model_encoder_type,\n",
    "        \"head_type\": transactions_model_head_type,\n",
    "        \"embedding_dropout\": 0.1\n",
    "    }\n",
    "    transactions_model = TransactionsModel(**transactions_model_config)\n",
    "\n",
    "    return transactions_model, projections_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Transactions model...\n",
      "USING whisper\n"
     ]
    }
   ],
   "source": [
    "transaction_model, projection_maps = load_transaction_model(head_type='next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PtlsPaddedBatch:\n",
    "    def __init__(self, data, mask):\n",
    "        self.payload = data\n",
    "        self.seq_lens = torch.LongTensor([data.shape[1]] * data.shape[0])\n",
    "        self.seq_len_mask = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PtlsEmbeddingLayer(EmbeddingLayer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_size = super().get_embedding_size()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask=x['mask']\n",
    "        x = super().forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_encoder = PtlsEmbeddingLayer(projection_maps['cat_embedding_projections'],\n",
    "                                    cat_features_names,\n",
    "                                    projection_maps['num_embedding_projections'],\n",
    "                                    num_features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = AlfaDataModule(data_dir='/home/jovyan/romashka/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resaving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('/home/jovyan/checkpoints/transactions_model/final_model.ckpt')\n",
    "# new_ckpt = {}\n",
    "# for elem in ckpt:\n",
    "#     if 'encoder' in elem:\n",
    "#         new_ckpt['encoder_model.' + elem] = ckpt[elem]\n",
    "#     elif 'head' in elem:\n",
    "#         new_ckpt['head.'+elem] = ckpt[elem]\n",
    "#     elif 'mapping_embedding' in elem:\n",
    "#         new_ckpt['connector.connector'+elem[len('mapping_embedding'):]] = ckpt[elem]\n",
    "#     else:\n",
    "#         new_ckpt[elem] = ckpt[elem]\n",
    "\n",
    "# torch.save(new_ckpt, '/home/jovyan/checkpoints/transactions_model/final_model_v2.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SampleSlices(split_count=7, cnt_min=10, cnt_max=30, is_sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.split_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dm = AlfaPretrainingDataModule(data_dir='/home/jovyan/romashka/data', rep=7, mode='coles', splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(new_dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    output = {}\n",
    "\n",
    "    # cat_features shape 1 x cat_features x seq_len\n",
    "    # num_features shape 1 x num_features x seq_len\n",
    "    # meta_feature shape 1 x meta_features\n",
    "    # mask shape 1 x seq_len\n",
    "    # label shape 1\n",
    "\n",
    "    # checking batch_size correctness\n",
    "    assert batch[0]['num_features'].shape[1] == 1, \"Incorrect output of dataloader\"\n",
    "\n",
    "    output['num_features'] = pad_sequence([d['num_features'].transpose(0, -1) for d in batch], # num_features x batch_size x seq_len\n",
    "                                            batch_first=True).squeeze(2).permute(-1, 0, 1)\n",
    "    output['cat_features'] = pad_sequence([d['cat_features'].transpose(0, -1) for d in batch], # cat_features x batch_size x seq_len\n",
    "                                            batch_first=True).squeeze(2).permute(-1, 0, 1)\n",
    "    output['meta_features'] = torch.cat([d['meta_features'] for d in batch], dim=1) # meta_features x batch_size\n",
    "\n",
    "    output['mask'] = pad_sequence([d['mask'].transpose(0, -1) for d in batch], batch_first=True).squeeze(2)\n",
    "    output['app_id'] = torch.cat([d['app_id'] for d in batch])\n",
    "\n",
    "    if 'label' in batch[0]:\n",
    "        output['label'] = torch.cat([d['label'] for d in batch])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_process(batch, splitter):\n",
    "    res = {}\n",
    "\n",
    "\n",
    "    seq_len = batch['mask'].shape[1]\n",
    "    local_date = torch.arange(seq_len)\n",
    "    if splitter is not None:\n",
    "        indexes = splitter.split(local_date)\n",
    "        pad_size = max([len(ixs) for ixs in indexes])\n",
    "    \n",
    "    for k, v in batch.items():\n",
    "        if k in ['num_features', 'cat_features'] and splitter is not None:\n",
    "            new_v = []\n",
    "            for elem in v:\n",
    "                tmp = []\n",
    "                for i, ixs in enumerate(indexes):\n",
    "                    to_tmp = elem[:, ixs]\n",
    "                    if to_tmp.shape[1] < pad_size:\n",
    "                        to_tmp = torch.cat([\n",
    "                            to_tmp, torch.zeros(to_tmp.shape[0], pad_size - to_tmp.shape[1], dtype=torch.int)\n",
    "                        ], axis=1)\n",
    "                    tmp.append(to_tmp)\n",
    "                new_v.append(torch.cat(tmp, dim=0))\n",
    "            new_v = torch.stack(new_v, dim=0)\n",
    "        elif k == 'meta_features' and splitter is not None:\n",
    "            new_v = v.repeat(1, len(indexes))\n",
    "        else:\n",
    "            new_v = v \n",
    "        res[k] = new_v\n",
    "    res['mask'] = res['cat_features'][0] != 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch, splitter, rep=7, mode='coles'):\n",
    "    batch = collate_fn(batch)\n",
    "    len_batch = batch['num_features'][0].shape[0]\n",
    "    labels = torch.arange(len_batch, device=batch['mask'].device).repeat(rep)\n",
    "    batch = split_process(batch, splitter)\n",
    "    \n",
    "    if mode == 'cpc':\n",
    "        return batch, None\n",
    "    elif mode == 'coles':\n",
    "        return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dm.val_ds,\n",
    "    batch_size=32,\n",
    "    collate_fn=partial(my_collate_fn, splitter=splitter,\n",
    "                       rep=7 if splitter is not None else 1,\n",
    "                       mode='coles')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingPlusConnector(nn.Module):\n",
    "    def __init__(self, embedding_layer, connector):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.connector = connector\n",
    "        self.output_size = connector.output_size\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        embedding = self.embedding_layer(batch)\n",
    "        embedding = self.connector(embedding, attention_mask=mask)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO split trx_encoder and seq_encoder\n",
    "\n",
    "class MySeqEncoder(SeqEncoderContainer):\n",
    "    def __init__(self,):\n",
    "\n",
    "        params =  {\n",
    "            'cat_embedding_projections': projection_maps['cat_embedding_projections'],\n",
    "            'cat_features': cat_features_names,\n",
    "            'num_embedding_projections': projection_maps['num_embedding_projections'],\n",
    "            'num_features': num_features_names,\n",
    "            'head_type': 'pretraining_last_output',\n",
    "            'encoder_type': 'whisper/tiny'\n",
    "        }\n",
    "        super().__init__(\n",
    "            trx_encoder=None,\n",
    "            seq_encoder_cls=TransactionsModel,\n",
    "            input_size=False,\n",
    "            seq_encoder_params=params,\n",
    "            is_reduce_sequence=False,\n",
    "        )\n",
    "        self.trx_encoder = EmbeddingPlusConnector(self.seq_encoder.embedding, self.seq_encoder.connector)\n",
    "                \n",
    "        self.full_model = self.seq_encoder\n",
    "        self.seq_encoder = self.seq_encoder.encoder_model\n",
    "        self.seq_encoder.embedding_size = self.seq_encoder.output_size\n",
    "    \n",
    "    def forward(self, x, h_0=None):\n",
    "        x = self.full_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING whisper\n"
     ]
    }
   ],
   "source": [
    "seq_encoder = MySeqEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoLESModule(\n",
    "    seq_encoder=seq_encoder,\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=1e-3),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=20, gamma=0.9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jovyan/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/pytorch_lightning/trainer/setup.py:179: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  category=PossibleUserWarning,\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.shared_step(*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/pytorch_lightning/loops/utilities.py:97: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  category=PossibleUserWarning,\n",
      "/home/jovyan/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:110: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  category=PossibleUserWarning,\n",
      "Missing logger folder: /home/jovyan/romashka/notebooks/lightning_logs\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | ContrastiveLoss | 0     \n",
      "1 | _seq_encoder       | MySeqEncoder    | 29.5 M\n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "29.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.5 M    Total params\n",
      "117.860   Total estimated model params size (MB)\n",
      "/home/jovyan/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:5: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  tensorboard.__version__\n",
      "/home/user/conda/lib/python3.7/site-packages/botocore/httpsession.py:28: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680\n",
      "  from urllib3.contrib.pyopenssl import orig_util_SSLContext as SSLContext\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:572: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.object, string),\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:573: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.bool, bool),\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:113: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:114: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n",
      "/home/jovyan/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed6d22345a54da5b8c701d9828ff1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff2a09b934b25be9a8b176e9d1e80ece927d491d3847c62b1a0ae23e4142515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
