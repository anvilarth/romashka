{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.transactions_qa.utils import get_projections_maps, get_exponent_number, get_mantissa_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.arange(10)\n",
    "buck = torch.arange(0, 10, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall rtdl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.bucketize(inp, buck, right=False)\n",
    "# masked_indices = \n",
    "\n",
    "num_buckets = len(buck) + 1\n",
    "\n",
    "new_indices = torch.clamp(indices -1, 0, num_buckets-2)\n",
    "\n",
    "matrix = torch.tril(torch.ones(num_buckets, num_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_matrix = matrix[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_sizes = buck.diff()\n",
    "buck_indices = torch.clamp(new_indices, 0, num_buckets - 3)\n",
    "size_down = bucket_sizes[buck_indices]\n",
    "\n",
    "adding = (inp - buck[new_indices]) / size_down\n",
    "mask_borders = (indices != 0) & (indices != num_buckets - 1)\n",
    "adding[~mask_borders] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = original_matrix.sum(1).long() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_matrix[torch.arange(10), mask_matrix] *= adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = nn.Linear(6, 385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x6 and 385x6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29841/4012548422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbasic_embedding\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0moriginal_matrix\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x6 and 385x6)"
     ]
    }
   ],
   "source": [
    "basic_embedding =  original_matrix @ weight.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3753,  0.3176, -0.3136,  ..., -0.1803,  0.3739, -0.0013],\n",
       "        [-0.2016,  0.2654, -0.3112,  ..., -0.0968,  0.2023,  0.0351],\n",
       "        [-0.0278,  0.2132, -0.3087,  ..., -0.0134,  0.0306,  0.0715],\n",
       "        ...,\n",
       "        [ 0.7058, -0.3745, -0.1778,  ...,  0.1686, -0.3010,  0.3373],\n",
       "        [ 0.6927, -0.4647, -0.0395,  ...,  0.1275, -0.3487,  0.1577],\n",
       "        [ 0.8186, -0.7694,  0.3162,  ...,  0.2795,  0.0012,  0.5489]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight(original_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basic_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29841/3458433188.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbasic_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'basic_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "basic_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseLinearEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, buckets):\n",
    "        super().__init__()\n",
    "        self.buckets = buckets \n",
    "        self.num_buckets = len(buckets) + 1\n",
    "\n",
    "        self.layer = nn.Linear(self.num_buckets, embedding_dim)\n",
    "        self.matrix = torch.tril(torch.ones(self.num_buckets, self.num_buckets))\n",
    "\n",
    "        self.bucket_sizes = self.buckets.diff()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        indices = torch.bucketize(x, self.buckets, right=False)\n",
    "        new_indices = torch.clamp(indices - 1, 0, self.num_buckets - 2)\n",
    "        original_matrix = self.matrix[indices]\n",
    "\n",
    "        buck_indices = torch.clamp(new_indices, 0, self.num_buckets - 3)\n",
    "        size_down = self.bucket_sizes[buck_indices]\n",
    "\n",
    "        adding = ((x - self.buckets[new_indices]) / size_down)\n",
    "\n",
    "        mask_borders = ((indices != 0) & (indices != self.num_buckets - 1))\n",
    "        adding[~mask_borders] = 1.0\n",
    "        mask_matrix = original_matrix.sum(1).long() - 1\n",
    "\n",
    "        original_matrix[torch.arange(batch_size), mask_matrix] *= adding\n",
    "\n",
    "        return self.layer(original_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.linspace(-3.5, 3.5, steps=200)\n",
    "buckets = torch.linspace(-3, 3, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PiecewiseLinearEmbedding(128, buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, buckets) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mantissa_embedding = PiecewiseLinearEmbedding(embedding_dim, buckets)\n",
    "        self.exponent_embedding = nn.Embedding(17, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        exponent = get_exponent_number(x) + 8\n",
    "        mantissa = get_mantissa_number(x)\n",
    "\n",
    "        embedding_exponent = self.exponent_embedding(exponent)\n",
    "        embedding_mantissa = self.mantissa_embedding(mantissa)\n",
    "\n",
    "        return torch.cat([embedding_mantissa, embedding_exponent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "numemb = NumEmbedding(385, buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 385])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numemb(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "questions = [\"Will the amount of the next transaction will be more than 0.54\", \"Will the amount of the next transaction will be more than 0.54\"]\n",
    "\n",
    "num_values = [re.findall(\"\\d+\\.\\d+\", question) for question in questions]\n",
    "numerics = [re.sub(\"\\d+\\.\\d+\", '<extra_id_0>', string) for string in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Will the amount of the next transaction will be more than <extra_id_0>',\n",
       " 'Will the amount of the next transaction will be more than <extra_id_0>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for value_list in num_values:\n",
    "    for value in value_list:\n",
    "        values.append(float(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5400, 0.5400])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqa_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29841/901446095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_input_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tqa_model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1035302448.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_25073/1035302448.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    torch.tensor(\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore, ROUGEScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds  = torch.round(torch.randn(10), decimals=2)\n",
    "targets =  torch.round(torch.randn(10), decimals=2)\n",
    "\n",
    "def transform_for_text_metrics(preds, targets):\n",
    "    new_preds = list(map(lambda x: str(round(x.item(), 2)).replace(\"\", \" \")[1: -1], preds))\n",
    "    new_targets = list(map(lambda x: str(round(x.item(), 2)).replace(\"\", \" \")[1: -1], targets))\n",
    "    return  new_preds, new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_fmeasure': tensor(0.8000),\n",
      " 'rouge1_precision': tensor(0.8000),\n",
      " 'rouge1_recall': tensor(0.8000),\n",
      " 'rouge2_fmeasure': tensor(0.5000),\n",
      " 'rouge2_precision': tensor(0.5000),\n",
      " 'rouge2_recall': tensor(0.5000),\n",
      " 'rougeL_fmeasure': tensor(0.8000),\n",
      " 'rougeL_precision': tensor(0.8000),\n",
      " 'rougeL_recall': tensor(0.8000),\n",
      " 'rougeLsum_fmeasure': tensor(0.8000),\n",
      " 'rougeLsum_precision': tensor(0.8000),\n",
      " 'rougeLsum_recall': tensor(0.8000)}\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "preds = [\"0 1 2 2 3\"]\n",
    "target = [\"0 2 2 2 3\"]\n",
    "rouge = ROUGEScore()\n",
    "from pprint import pprint\n",
    "pprint(rouge(preds, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def text2float(textnum, decimal='dot'):\n",
    "    numwords = {}\n",
    "\n",
    "    units = [ \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\",\n",
    "            \"seven\", \"eight\", \"nine\"]\n",
    "            \n",
    "    bigger_units =[\"ten\", \"eleven\", \"twelve\",\n",
    "            \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\",\n",
    "            \"eighteen\", \"nineteen\"]\n",
    "\n",
    "    tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \n",
    "            \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "    scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\", \n",
    "            'quadrillion', 'quintillion', 'sexillion', 'septillion', \n",
    "            'octillion', 'nonillion', 'decillion' ]\n",
    "\n",
    "    numwords[\"and\"] = (1, 0)\n",
    "    for idx, word in enumerate(units+bigger_units): numwords[word] = (1, idx)\n",
    "    for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)\n",
    "    for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, \n",
    "            'eighth':8, 'ninth':9, 'twelfth':12}\n",
    "    ordinal_endings = [('ieth', 'y'), ('th', '')]\n",
    "    current = result = 0\n",
    "    \n",
    "\n",
    "    if \"dot\"  in textnum:\n",
    "        first, second = re.split(decimal, textnum)\n",
    "        first_tokens = re.split(r\"[\\s-]+\", first.rstrip())\n",
    "        second_tokens = re.split(r\"[\\s-]+\", second.lstrip())\n",
    "    else:\n",
    "        first_tokens = re.split(r\"[\\s-]+\", textnum)\n",
    "        \n",
    "    for word in first_tokens:\n",
    "        if word in ordinal_words:\n",
    "            scale, increment = (1, ordinal_words[word])\n",
    "        else:\n",
    "            for ending, replacement in ordinal_endings:\n",
    "                if word.endswith(ending):\n",
    "                    word = \"%s%s\" % (word[:-len(ending)], replacement)\n",
    "\n",
    "            if word not in numwords:\n",
    "                raise Exception(\"Illegal word: \" + word)\n",
    "\n",
    "            scale, increment = numwords[word]\n",
    "\n",
    "        if scale > 1:\n",
    "            current = max(1, current)\n",
    "\n",
    "        current = current * scale + increment\n",
    "        if scale > 100:\n",
    "            result += current\n",
    "            current = 0\n",
    "\n",
    "    result += current\n",
    "\n",
    "    if \"dot\" in textnum:\n",
    "        multi = 0.1\n",
    "        for word in second_tokens:\n",
    "            if word not in units:\n",
    "                raise Exception(\"Illegal word in fractional: \" + word)\n",
    "            scale, increment = numwords[word]\n",
    "            result += multi * increment\n",
    "            multi *= 0.1\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ff2a09b934b25be9a8b176e9d1e80ece927d491d3847c62b1a0ae23e4142515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
