{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra \n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.register_new_resolver(\"eval\", eval)\n",
    "cfg = OmegaConf.load('../configs/train.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Hydra\n",
    "initialize('../configs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name='train.yaml', overrides=['paths.root_dir=/home/jovyan/romashka', 'task=next_transactions_30_days_multi.yaml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING whisper\n"
     ]
    }
   ],
   "source": [
    "datamodule = hydra.utils.instantiate(cfg.data)\n",
    "\n",
    "\n",
    "model = hydra.utils.instantiate(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Too many dataloader workers: 10 (max is dataset.n_shards=5). Stopping 5 dataloader workers.\n",
      "To parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary to have a number of workers greater than dataset.n_shards=5. To enable more parallelism, please split the dataset in more files than 5.\n"
     ]
    }
   ],
   "source": [
    "itert = iter(datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(itert)\n",
    "# batch = next(itert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/jovyan/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/pytorch_lightning/core/module.py:417: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  \"You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet.\"\n",
      "2309it [03:50, 10.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4823/2960407962.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/romashka/src/models/task_model.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/romashka/src/models/task_model.py\u001b[0m in \u001b[0;36mshared_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/romashka/src/models/task_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransactions_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshared_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/romashka/src/models/components/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, embeds)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/romashka/src/models/components/models.py\u001b[0m in \u001b[0;36mget_embs\u001b[0;34m(self, batch, embeds)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/romashka/src/models/components/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embedding, mask)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'decision-transformer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bert'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's2t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'whisper'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# input_ids = self.construct_positional_embedding(embedding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m't5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m                 )\n\u001b[1;32m   1106\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         )\n\u001b[1;32m    529\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.imgenv-afilatov-dev-8-0/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(datamodule.train_dataloader()):\n",
    "        tmp = model.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4695, -0.0974,  0.2264, -0.9167,  0.5797,  1.6664, -0.1064, -0.1446,\n",
       "          -0.4268, -0.5517, -0.8987,  0.1579, -0.3261, -0.3474, -0.8121, -0.5303,\n",
       "           0.4025, -0.2515,  0.2549, -1.1754,  0.2315, -0.0522,  1.0249,  1.0697,\n",
       "           0.9959, -0.6238,  0.1677,  0.3477],\n",
       "         [-0.5089,  0.0994, -0.0032, -0.9258,  0.8807,  1.4777,  0.1614, -0.1338,\n",
       "          -0.4179, -0.5655, -0.7741,  0.2963, -0.3608, -0.4179, -0.6660, -0.5894,\n",
       "           0.3418, -0.3520,  0.1697, -1.4000,  0.4475,  0.0395,  0.8586,  0.7886,\n",
       "           0.9112, -0.3691,  0.1588,  0.2652],\n",
       "         [-0.5522,  0.0616,  0.1328, -1.1728,  0.9140,  1.2957,  0.0932, -0.0879,\n",
       "          -0.2851, -0.5518, -0.8004,  0.2094, -0.3755, -0.3431, -0.6335, -0.6289,\n",
       "           0.3646, -0.3248,  0.1496, -1.2392,  0.2172, -0.1499,  0.8108,  1.0352,\n",
       "           1.1409, -0.6312,  0.0685,  0.1659],\n",
       "         [-0.5867, -0.1452,  0.1947, -0.9861,  0.6442,  1.5768, -0.1849, -0.1366,\n",
       "          -0.5148, -0.5209, -0.8967,  0.2373, -0.2010, -0.4235, -0.8319, -0.5444,\n",
       "           0.3594, -0.4291,  0.2031, -1.1861,  0.1489, -0.0587,  0.8628,  1.1253,\n",
       "           1.0445, -0.5805,  0.0916,  0.1660],\n",
       "         [-0.6888, -0.0791,  0.1356, -1.0539,  0.4728,  1.4918, -0.0291, -0.0925,\n",
       "          -0.5034, -0.3730, -0.7490,  0.0564, -0.3613, -0.3218, -0.7645, -0.4613,\n",
       "           0.4509, -0.4270,  0.1669, -1.4523,  0.2615, -0.0030,  1.0835,  1.0120,\n",
       "           1.0263, -0.6042,  0.1394,  0.2477],\n",
       "         [-0.6620, -0.1321,  0.2544, -1.1943,  0.6432,  1.4623,  0.0641,  0.0346,\n",
       "          -0.2875, -0.5294, -0.8725,  0.1848, -0.3765, -0.1177, -0.7508, -0.6626,\n",
       "           0.4958, -0.4316,  0.3691, -1.4576,  0.3247,  0.1883,  0.9490,  0.9490,\n",
       "           1.0608, -0.3588,  0.1037,  0.0922],\n",
       "         [-0.6505,  0.0997,  0.1180, -1.0704,  0.8598,  1.5796,  0.1760, -0.0801,\n",
       "          -0.2300, -0.3652, -0.8661,  0.0590, -0.4806, -0.3389, -0.6268, -0.7456,\n",
       "           0.4336, -0.4555,  0.0520, -1.2477,  0.2216,  0.0103,  1.0011,  0.8931,\n",
       "           0.8720, -0.3413,  0.3229,  0.2555],\n",
       "         [-0.6834, -0.0557,  0.2183, -1.0490,  0.7042,  1.4865,  0.1530, -0.1732,\n",
       "          -0.2657, -0.4834, -0.9734,  0.1044, -0.3188, -0.3395, -0.8277, -0.6960,\n",
       "           0.4193, -0.3515,  0.2914, -1.4400,  0.2967,  0.1714,  1.0418,  0.9473,\n",
       "           1.0377, -0.3574,  0.1883,  0.2149],\n",
       "         [-0.6790, -0.1294,  0.2420, -1.0402,  0.7280,  1.3429,  0.0928, -0.1695,\n",
       "          -0.3086, -0.4248, -0.8525,  0.2060, -0.3375, -0.2865, -0.8135, -0.5752,\n",
       "           0.3241, -0.3737,  0.0741, -1.3530,  0.4059,  0.0437,  0.9390,  0.8197,\n",
       "           0.9097, -0.3784,  0.0351,  0.2338],\n",
       "         [-0.6483, -0.1438,  0.0559, -1.0139,  0.5928,  1.4443, -0.0223, -0.0451,\n",
       "          -0.4092, -0.3684, -0.7556,  0.1395, -0.3762, -0.3244, -0.7834, -0.4322,\n",
       "           0.4342, -0.4246,  0.1899, -1.3359,  0.2136, -0.0646,  1.1278,  0.9933,\n",
       "           0.9853, -0.6135,  0.2194,  0.2685],\n",
       "         [-0.5124, -0.1312,  0.2221, -1.0485,  0.7146,  1.6157, -0.1008, -0.0908,\n",
       "          -0.3857, -0.6511, -0.9356,  0.1111, -0.2338, -0.4271, -0.8470, -0.5297,\n",
       "           0.3344, -0.3997,  0.3014, -1.1688,  0.1494, -0.0787,  0.9420,  1.1593,\n",
       "           0.9321, -0.4912,  0.0916,  0.1586],\n",
       "         [-0.5888, -0.0943,  0.0677, -0.9955,  0.7780,  1.3152,  0.0561, -0.0958,\n",
       "          -0.3096, -0.4751, -0.6105,  0.1840, -0.2419, -0.2771, -0.9030, -0.5685,\n",
       "           0.4395, -0.3135,  0.1038, -1.1867,  0.3353, -0.0378,  0.9898,  0.9855,\n",
       "           0.9181, -0.7250,  0.1969,  0.1907],\n",
       "         [-0.5722, -0.2188,  0.1167, -0.8793,  0.4506,  1.6019,  0.0742, -0.2127,\n",
       "          -0.4769, -0.3990, -0.7903,  0.1332, -0.2880, -0.2788, -0.9247, -0.6112,\n",
       "           0.4133, -0.4115,  0.2988, -1.2789,  0.2700,  0.0276,  0.9995,  1.0930,\n",
       "           0.9723, -0.6997,  0.0756,  0.2123],\n",
       "         [-0.7946,  0.0114,  0.1075, -1.1351,  0.7588,  1.4985,  0.0489, -0.0218,\n",
       "          -0.3145, -0.4388, -0.8322,  0.0176, -0.3028, -0.2964, -0.6935, -0.5903,\n",
       "           0.3880, -0.4213,  0.0534, -1.3240,  0.1961, -0.1499,  0.8826,  0.9861,\n",
       "           0.9100, -0.6169,  0.1003,  0.1047],\n",
       "         [-0.7217, -0.0807,  0.2555, -1.1412,  0.5708,  1.4241,  0.0648, -0.1427,\n",
       "          -0.4536, -0.3609, -0.8292,  0.0943, -0.3564, -0.2745, -0.9160, -0.5291,\n",
       "           0.4283, -0.4164,  0.1710, -1.4424,  0.2987, -0.1067,  1.0207,  1.0114,\n",
       "           1.0107, -0.5249,  0.1716,  0.1013],\n",
       "         [-0.6572, -0.1186,  0.2040, -1.1409,  0.8077,  1.3133,  0.0793, -0.1062,\n",
       "          -0.4077, -0.5482, -0.7799,  0.1678, -0.2363, -0.2344, -0.8463, -0.4981,\n",
       "           0.3278, -0.3653,  0.1966, -1.3374,  0.2971,  0.0171,  0.9646,  0.9447,\n",
       "           1.0335, -0.5517,  0.0841,  0.0841],\n",
       "         [-0.6933,  0.0630,  0.2386, -0.9948,  0.7382,  1.5031,  0.0092, -0.0882,\n",
       "          -0.4906, -0.4044, -0.9578,  0.1390, -0.4550, -0.2951, -0.6711, -0.5092,\n",
       "           0.5111, -0.5632,  0.0422, -1.4671,  0.2159, -0.2063,  0.8805,  1.0341,\n",
       "           1.2792, -0.4773,  0.0427,  0.2742],\n",
       "         [-0.7020, -0.1536,  0.1277, -1.1091,  0.6502,  1.5377,  0.0260, -0.0630,\n",
       "          -0.2707, -0.3263, -0.7652, -0.0031, -0.3845, -0.2676, -0.7913, -0.5021,\n",
       "           0.3695, -0.4183,  0.1553, -1.2006,  0.2754,  0.0072,  1.0400,  0.9064,\n",
       "           0.8652, -0.5771,  0.0679,  0.1545],\n",
       "         [-0.5946,  0.0088,  0.1900, -1.0711,  0.7996,  1.5106,  0.1348, -0.1231,\n",
       "          -0.2776, -0.3789, -0.9137,  0.0631, -0.3803, -0.3700, -0.7027, -0.5812,\n",
       "           0.3153, -0.4981,  0.1374, -1.3700,  0.3718,  0.0285,  0.9502,  0.8914,\n",
       "           0.9714, -0.2979,  0.0934,  0.2964],\n",
       "         [-0.6560,  0.0769,  0.2340, -1.1925,  0.6924,  1.4674,  0.1313, -0.1759,\n",
       "          -0.3577, -0.3959, -0.8244,  0.2103, -0.4316, -0.3024, -0.8356, -0.5093,\n",
       "           0.3409, -0.3040,  0.1686, -1.4927,  0.2547,  0.0224,  0.9902,  1.0399,\n",
       "           1.2175, -0.4478, -0.0641,  0.1136],\n",
       "         [-0.5649, -0.0995,  0.2409, -1.1203,  0.6219,  1.4734,  0.0092,  0.0700,\n",
       "          -0.3236, -0.4142, -0.9275,  0.1031, -0.3460, -0.2119, -0.7477, -0.5511,\n",
       "           0.3540, -0.2795,  0.2948, -1.4018,  0.2702,  0.1281,  0.9085,  0.9840,\n",
       "           1.0027, -0.3904,  0.1151,  0.1800],\n",
       "         [-0.5379,  0.0042,  0.2116, -1.1230,  0.7884,  1.6146, -0.0725, -0.0666,\n",
       "          -0.4697, -0.3998, -0.8697,  0.1994, -0.4497, -0.3079, -0.7548, -0.5133,\n",
       "           0.4434, -0.4038, -0.0092, -1.2487,  0.2661, -0.0827,  0.9874,  0.9720,\n",
       "           0.9788, -0.4262,  0.1103,  0.3748],\n",
       "         [-0.6375, -0.0040,  0.1184, -1.1017,  0.8440,  1.4590,  0.1955, -0.1063,\n",
       "          -0.2766, -0.6732, -0.9144,  0.0782, -0.3325, -0.3087, -0.5776, -0.5682,\n",
       "           0.6125, -0.5763,  0.2887, -1.4285,  0.2196,  0.0080,  0.8502,  0.9657,\n",
       "           0.9544, -0.5126,  0.1093,  0.0076],\n",
       "         [-0.6802,  0.1641,  0.0873, -1.1341,  0.6820,  1.3979,  0.1322,  0.0438,\n",
       "          -0.2152, -0.4839, -0.8427, -0.0382, -0.4392, -0.2954, -0.6628, -0.5997,\n",
       "           0.4542, -0.6241,  0.2299, -1.2943,  0.2992, -0.0527,  0.7740,  1.0252,\n",
       "           0.9699, -0.3488,  0.1213,  0.0239],\n",
       "         [-0.6857,  0.0051,  0.0622, -1.0183,  0.7572,  1.4888,  0.0921, -0.0888,\n",
       "          -0.3614, -0.4828, -0.8117,  0.1307, -0.2587, -0.2540, -0.6923, -0.4218,\n",
       "           0.3950, -0.4911,  0.1873, -1.3511,  0.2517, -0.0925,  0.9077,  1.1360,\n",
       "           0.9920, -0.6025,  0.0059,  0.0922],\n",
       "         [-0.7877,  0.1489,  0.2061, -1.2444,  0.7803,  1.5514,  0.1459, -0.0436,\n",
       "          -0.2431, -0.5242, -0.9041,  0.0631, -0.4194, -0.3209, -0.5743, -0.6310,\n",
       "           0.4523, -0.5319,  0.2487, -1.2798,  0.2901, -0.1003,  0.6725,  1.1665,\n",
       "           1.1868, -0.4143, -0.0464, -0.0676],\n",
       "         [-0.6506, -0.0384,  0.2213, -1.1597,  0.7325,  1.6527,  0.0332, -0.0972,\n",
       "          -0.5114, -0.4992, -0.8329,  0.1569, -0.3967, -0.2637, -0.7367, -0.4702,\n",
       "           0.4184, -0.5295,  0.1517, -1.3757,  0.2260, -0.1233,  1.0007,  1.1455,\n",
       "           1.1126, -0.5915, -0.0730,  0.1164],\n",
       "         [-0.6356,  0.0163, -0.0348, -1.1074,  0.8081,  1.5630,  0.0802, -0.0710,\n",
       "          -0.3190, -0.5420, -0.8162,  0.0062, -0.3368, -0.2668, -0.6288, -0.4604,\n",
       "           0.4759, -0.5153,  0.2162, -1.2850,  0.2655, -0.0606,  0.8029,  1.1718,\n",
       "           0.9522, -0.5076, -0.0390,  0.1373],\n",
       "         [-0.5985, -0.0734,  0.2096, -1.1814,  0.7583,  1.6127,  0.0750, -0.0770,\n",
       "          -0.2553, -0.3558, -0.8106,  0.1722, -0.4784, -0.2695, -0.7356, -0.6146,\n",
       "           0.3924, -0.3414,  0.1002, -1.2740,  0.4036,  0.0407,  0.9728,  0.9450,\n",
       "           0.9423, -0.3235,  0.1521,  0.2036],\n",
       "         [-0.6112, -0.2021,  0.1614, -1.1102,  0.6584,  1.5123,  0.1053, -0.1059,\n",
       "          -0.3079, -0.5016, -0.7700,  0.1505, -0.2864, -0.3028, -0.8435, -0.6273,\n",
       "           0.3868, -0.4551,  0.2935, -1.2634,  0.2570,  0.0501,  0.8889,  0.9899,\n",
       "           1.0209, -0.4839,  0.0783,  0.0709],\n",
       "         [-0.5416, -0.0558,  0.0664, -1.0092,  0.6101,  1.6583,  0.0342, -0.0324,\n",
       "          -0.2489, -0.5602, -0.8182,  0.0563, -0.3550, -0.3726, -0.7544, -0.7363,\n",
       "           0.4356, -0.3333,  0.3368, -1.1139,  0.3440,  0.0572,  1.0356,  1.1629,\n",
       "           0.8801, -0.4993,  0.2316,  0.1839],\n",
       "         [-0.5271,  0.0332,  0.0946, -1.0376,  0.7493,  1.5664,  0.0202, -0.0234,\n",
       "          -0.5438, -0.3430, -0.7544,  0.1695, -0.5043, -0.3726, -0.6960, -0.5312,\n",
       "           0.4692, -0.2949,  0.2517, -1.3800,  0.2596, -0.0413,  1.2582,  1.0467,\n",
       "           1.0591, -0.5619,  0.3529,  0.2241]], grad_fn=<AddmmBackward0>),\n",
       " tensor([ 4,  1,  2,  7,  4,  2,  2, 11,  1, 11,  7,  3,  2,  4,  2,  6,  1, 14,\n",
       "          1, 20, 15,  1,  2,  2,  1,  2, 21,  1,  9,  3,  6,  1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shared_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs, answers = model.shared_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16317it [00:51, 319.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# targets = []\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     for batch in tqdm.tqdm(datamodule.train_dataloader()):\n",
    "#         qa_batch = model.task.prepare_task_batch(batch)\n",
    "#         if qa_batch:\n",
    "#             targets.extend(qa_batch['label'].tolist())\n",
    "\n",
    "\n",
    "# np.mean(targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff2a09b934b25be9a8b176e9d1e80ece927d491d3847c62b1a0ae23e4142515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
